{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 3: Digit Dataset (A)\n",
    "In this example we will work on <strong> Digit Recognition </strong>.<br>\n",
    "This Dataset contains for each image the values of its $64=8 \\times 8$ <strong>greyscale pixels</strong> and the value of the <strong>Digit</strong> <br>\n",
    "<p style=\"text-align:center;\"><img src=\"Example3/digits_image.png\" class=\"center\"></p>\n",
    "\n",
    "### NOTE\n",
    "To execute a cell, press <strong>Shift+Enter</strong>\n",
    "\n",
    "## 0. Helper Class & Creating custom Images\n",
    "<code> ImagePredictor </code> is a class that facilates predicting photos <br>\n",
    "The following images are custom one that will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "class ImagePredictor:\n",
    "    def __init__(self,model,dim,max_val=255):\n",
    "        self.model=model\n",
    "        self.dim=dim\n",
    "        self.max_val=max_val\n",
    "        pass\n",
    "    \n",
    "    def fit(X,y):\n",
    "        model.fit(np.reshape(X,[X.shape[0],dim[0]*dim[1]]),y)\n",
    "        return self\n",
    "#Predict a Single Image\n",
    "    def predict_one(self,img,pix_max_val=255,invert=False):\n",
    "        img2=np.copy(img)\n",
    "        img2 = img2.astype(np.float64)\n",
    "        img2=cv2.resize(img2,self.dim)\n",
    "        img2= img2*self.max_val/pix_max_val\n",
    "        if invert:\n",
    "            img2=self.max_val-img2 \n",
    "        return self.model.predict([img2.flatten()])[0]\n",
    "#Predict an array of images (a 3D Array)\n",
    "    def predict(self,img,pix_max_val=255,invert=False):\n",
    "        img=img.astype(np.float64)\n",
    "        img2 = np.zeros([img.shape[0],*self.dim])\n",
    "        for k in range(img.shape[0]):\n",
    "            img2[k,:,:]=cv2.resize(img[k,:,:],self.dim)\n",
    "            img2[k,:,:]= img2[k,:,:]*self.max_val/pix_max_val\n",
    "            if invert:\n",
    "                img2[k,:,:]=self.max_val-img2[k,:,:]\n",
    "#        return np.array([self.predict_one(img2[s,:,:]) for s in range(img.shape[0]) ])\n",
    "        return self.model.predict(np.reshape(img2,[img2.shape[0],self.dim[0]*self.dim[1]]))\n",
    "    def score(self,X,y,pix_max_val=255,invert=False):\n",
    "        return (self.predict(X,pix_max_val,invert)==y).mean()\n",
    "    \n",
    "    def unroll(img,pix_max_val=255,invert=False):\n",
    "        img2 = np.zeros([img.shape[0],*self.dim])\n",
    "        for k in range(img.shape[0]):\n",
    "            img2[k,:,:]=cv2.resize(img[k,:,:],self.dim)\n",
    "            img2[k,:,:]= img2[k,:,:]*self.max_val/pix_max_val\n",
    "            if invert:\n",
    "                img2[k,:,:]=self.max_val-img2[k,:,:]\n",
    "        return img2\n",
    "        \n",
    "    pass\n",
    "\n",
    "#The list of file names for the custom images\n",
    "file_name_list=[\"2.png\",\"3.png\",\"8.jpg\",\"4.bmp\",\"5.bmp\",\"5_2.bmp\",\"6.bmp\",\"7.bmp\",\"7.jpg\",\"9.bmp\",\"9.jpg\",\"8.bmp\"]\n",
    "#The digit value of each image, in order\n",
    "digit_values=np.array([2,3,8,4,5,5,6,7,7,9,9,8])\n",
    "#Creating a 3D array(Tensor) of 28x28 images\n",
    "custom_images = np.array([cv2.resize(cv2.imread(\"Example3/{}\".format(s),cv2.IMREAD_GRAYSCALE),(28,28)) \n",
    "        for s in file_name_list])\n",
    "print(\"Good Job!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Libraries\n",
    "We will need \n",
    "- numpy (NumPy) for arrays\n",
    "- pandas (Pandas) for data manipulation\n",
    "- matplotlib for visualisation\n",
    "- sklearn (Scikit-Learn) for creating, fitting and evaluating the model\n",
    "- seaborn (Seaborn), which gives a simpler syntax for visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sklearn.datasets contains some predefined datasets\n",
    "import sklearn.datasets as ds\n",
    "#pandas is used for data manipulation\n",
    "import pandas as pd\n",
    "#matplotlib.pyplot is used for visualisation\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as grd\n",
    "#seaborn is a user friendly library for visualisation built on top of matplotlib\n",
    "import seaborn as sns\n",
    "#we will use a Classification Model called SVC: Support Vector Classifier\n",
    "import sklearn.datasets as ds\n",
    "from sklearn.svm import SVC\n",
    "#we will compare it to a linear Model: LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#we will split the data using train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "#OPTIONAL: This is for evaluating a classification model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Libraries are Imported\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preprocessing: Loading Digits Dataset\n",
    "We will Load here The Digits Dataset <br>\n",
    "We will then Load the $X$ values and $y$ values<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the Digits dataset\n",
    "digits_dataset=ds.load_digits()\n",
    "#X is a DataFrame (Matrix/2D array) containing the pixels of each digit in the dataset \n",
    "X=pd.DataFrame(digits_dataset[\"data\"])\n",
    "#y is the digit itself for every image, respecting the order \n",
    "#y is a vector (Series/1D array)\n",
    "y=pd.Series(digits_dataset[\"target\"],name =\"digit\")\n",
    "feature_names = digits_dataset[\"feature_names\"]\n",
    "X.columns=feature_names\n",
    "print(\"Data Successfully Preprocessed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an example of unrolling a list\n",
    "#The unrolled list is X.shape\n",
    "print(\"This Dataset contains {} obersvation(s)\\nEach observation has {} feature(s)\".format(*X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing a sample of $X$\n",
    "Feel free to change the number of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Viewing the first elements of $y$\n",
    "Feel free to change the number below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analysing Data\n",
    "### Merging Tables\n",
    "To Analyse data, sometimes, it maybe simpler to combine the data to one table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U=pd.concat([X,y],axis=1)\n",
    "#To see the last 10 examples\n",
    "U.tail(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plotting digits\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "#A grid spec is a grid layout\n",
    "spec = grd.GridSpec(ncols=3,nrows=4)\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        d=3*i+j\n",
    "        ax = fig.add_subplot(spec[i,j])\n",
    "        digit_matrix=np.reshape(X.iloc[d,:].to_numpy(),[8,8])\n",
    "        ax=sns.heatmap(digit_matrix,cmap=\"gray\",ax=ax,cbar=False,xticklabels=False,yticklabels=False)\n",
    "        ax.set_title(\"$Digit: {}$\".format(y[d]))\n",
    "ax = fig.add_subplot(spec[3,1])\n",
    "digit_matrix=np.reshape(X.iloc[9,:].to_numpy(),[8,8])\n",
    "ax=sns.heatmap(digit_matrix,cmap=\"gray\",ax=ax,cbar=False,xticklabels=False,yticklabels=False)\n",
    "ax.set_title(\"$Digit: {}$\".format(y[9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Selection\n",
    "### Creating Train & Test Sets\n",
    "We will Create a training set that is used to fit our model<br>\n",
    "The training data is a random sample of size $70\\%$ of the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is an example of multiple return values in Python\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,train_size=.7)\n",
    "print(\"Train/Test Data Successfully created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating & Fitting Model\n",
    "Here we will create two models: \n",
    "1. a LogisticRegression model\n",
    "2. an SVC model (Support Vector Classifier)\n",
    "\n",
    "We will fit each model against the <strong>training data</strong><br>\n",
    "Feel free to change the constant $C>0$ of each model and see how it affects accuarcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating SVC Model\n",
    "svc_model = SVC(C=1)\n",
    "#Fitting Model\n",
    "svc_model.fit(X_train,y_train)\n",
    "\n",
    "#Creating LogisticRegression Model\n",
    "linear_model =LogisticRegression(C=1)\n",
    "#fitting linear model\n",
    "linear_model.fit(X_train,y_train)\n",
    "print(\"Model is now Fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluating Model\n",
    "### a. Evaluating on the Testing Dataset\n",
    "We will evaluate the accuarcy of our model with the <strong>testing data</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function evaluates a list of models\n",
    "#models is the list of models, models_names is the list of the names of each model\n",
    "def evaluate_models(models,models_names,X_test,y_test):\n",
    "    fig,axs= plt.subplots(1,len(models),figsize=(15,6))\n",
    "    k=0\n",
    "    for model,name in zip(models,models_names):\n",
    "        accuarcy=model.score(X_test,y_test)\n",
    "        print(\"{} Model has an accuarcy of {:.3f}%\".format(name,100*accuarcy))\n",
    "        axs[k]\n",
    "        sns.heatmap(confusion_matrix(y_test,model.predict(X_test)),annot=True,ax=axs[k])\n",
    "        axs[k].set_title(\"{}: Relation Between Predicted Values & Correct Values\".format(name))\n",
    "        axs[k].set_ylabel(\"Correct Values\")\n",
    "        axs[k].set_xlabel(\"Predicted Values\");\n",
    "        k=k+1\n",
    "\n",
    "#This call will evaluate both models\n",
    "evaluate_models([linear_model,svc_model],[\"Linear Model\",\"SVC\"],X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Testing on other custom Images\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><img src=\"Example3/0.bmp\" width=\"32\" height=\"32\"></td>         \n",
    "        <td><img src=\"Example3/2.png\" width=\"32\" height=\"32\"></td>         \n",
    "        <td><img src=\"Example3/3.png\" width=\"32\" height=\"32\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"Example3/4.bmp\" width=\"32\" height=\"32\"></td>         \n",
    "        <td><img src=\"Example3/5.bmp\" width=\"32\" height=\"32\"></td>         \n",
    "        <td><img src=\"Example3/5_2.bmp\" width=\"32\" height=\"32\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"Example3/6.bmp\" width=\"32\" height=\"32\"></td>         \n",
    "        <td><img src=\"Example3/7.bmp\" width=\"32\" height=\"32\"></td>         \n",
    "        <td><img src=\"Example3/7.jpg\" width=\"32\" height=\"32\"></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"Example3/8.bmp\" width=\"32\" height=\"32\"></td>         \n",
    "        <td><img src=\"Example3/9.bmp\" width=\"32\" height=\"32\"></td>         \n",
    "        <td><img src=\"Example3/9.bmp\" width=\"32\" height=\"32\"></td>\n",
    "    </tr>\n",
    "</table>\n",
    "Well, such results may be outstandingly good. But does it generalise well to <strong>new data?</strong><br>\n",
    "We will test this <strong>model</strong> on some <strong>randomly</strong> chosen digit images from <strong>the internet.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Select a model from the two defined above\n",
    "predictor= ImagePredictor(svc_model,(8,8),16)\n",
    "print(\"The accuarcy of the selected model is {:.3f}\".format(predictor.score(custom_images,digit_values,invert=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improving Digit Prediction: MNIST Dataset (B)\n",
    "- This is way more <strong>advanced</strong> than what <strong>we have seen</strong>, so prepare yourself.\n",
    "- We <strong>recommend</strong> that you give it <strong>another try</strong> at your <strong>free time</strong>.<br>\n",
    "In this example we will work on Digit Recognition with the <strong>MNIST Dataset</strong>.<br>\n",
    "<p style=\"text-align:center;\"><img src=\"Example3/mnist_image.png\"></p><br>\n",
    "This Dataset contains for each image the values of its  784=28Ã—28  greyscale pixels and the value of the Digit\n",
    "\n",
    "## 1. Importing MNIST\n",
    "### Reading from a CSV file\n",
    "The code below imports the <strong>MNIST dataset</strong> (both training and testing) from csv files.<br>\n",
    "It then it shows a sample from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train=pd.read_csv(\"Example3/mnist_train.csv\")\n",
    "mnist_test=pd.read_csv(\"Example3/mnist_test.csv\")\n",
    "mnist_train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating $X$ & $y$ arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mnist_train = mnist_train[mnist_train.columns[1:]]\n",
    "y_mnist_train=mnist_train[\"label\"]\n",
    "X_mnist_test = mnist_test[mnist_test.columns[1:]]\n",
    "y_mnist_test=mnist_test[\"label\"]\n",
    "#To Save Some memory\n",
    "mnist_train=None\n",
    "mnist_test=None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Viewing some digits\n",
    "Feel free to change <code>ncols</code> and <code>nrows</code>, we recommend you set each one of them between $1$ & $5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ploting MNIST digits\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "#This is the number of columns of the plot\n",
    "ncols=5\n",
    "#This is the number of rows of the plot\n",
    "nrows=4\n",
    "spec = grd.GridSpec(ncols=ncols,nrows=nrows)\n",
    "X_mnist_sample=X_mnist_train.sample(nrows*ncols)\n",
    "y_mnist_sample=y_mnist_train[X_mnist_sample.index]\n",
    "for i in range(nrows):\n",
    "    for j in range(ncols):\n",
    "        d=3*i+j\n",
    "        ax = fig.add_subplot(spec[i,j])\n",
    "        digit_matrix=np.reshape(X_mnist_sample.iloc[d,:].to_numpy(),[28,28])\n",
    "        ax=sns.heatmap(digit_matrix,cmap=\"gray\",ax=ax,cbar=False,xticklabels=False,yticklabels=False)\n",
    "        ax.set_title(\"$Digit: {}$\".format(y_mnist_sample.iloc[d]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating MNIST Models\n",
    "This can be considered an improvement of Example 3<br>\n",
    "### a. Model 1: <strong>a Pipeline</strong> (Classical Machine Learning)\n",
    "We will use a <strong>pipeline</strong> composed of a:\n",
    "- scaler: StandardScaler To scale data\n",
    "- dimensionality_reduction: PCA to (in some sense) remove non-necessary variables/pixels\n",
    "- predictor: SVC to fit and predict data\n",
    "\n",
    "this model will automatically scale data before every operation, which will ensure better performance especially for SVC models.<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A pipeline is a way to connect models to create a more powerful model\n",
    "from sklearn.pipeline import Pipeline\n",
    "#A StandardScaler is a model that outputs the dataset scaled: with 0 mean and unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#A PCA: Principal Component Analysis, It lets you to extract the direction which are the most used for predictions\n",
    "from sklearn.decomposition import PCA\n",
    "mnist_model = Pipeline([(\"scaler\",StandardScaler()),(\"dimensionality_reduction\",PCA(n_components=32)),(\"predictor\",SVC(C=1))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Model 2: <strong>Convolutional Neural Network</strong>\n",
    "- A <strong>Convolutional Neural Network</strong> Is a neural network with some convolution layers\n",
    "- A <strong>convolution</strong> is an <strong>operation</strong> on <strong>image</strong> which <strong>gives</strong> an other <strong>image</strong>. It is usually used for sharpening, blurring and edge detection.\n",
    "<br>\n",
    "<p style=\"text-align:center\">\"<img src=\"Example3/convolution_image.jpg\"></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from keras.utils import to_categorical\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "deep_model = tf.keras.Sequential([\n",
    "#A convolution: for edge detection (hopefully)\n",
    "    tf.keras.layers.Conv2D(32,3,activation=\"relu\",input_shape=(28,28,1),kernel_initializer=\"he_uniform\"),\n",
    "#Normalizing Data, similar to StandardScaler in classical machine learning\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "#Max pooling layer, to extract the pixel with maximum edge value\n",
    "    tf.keras.layers.MaxPooling2D((2,2)),\n",
    "#Converting each matrix (image) to vector \n",
    "    tf.keras.layers.Flatten(),\n",
    "#From now we have a standard neural network\n",
    "    tf.keras.layers.Dense(100,activation=\"relu\",kernel_initializer=\"he_uniform\"),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(10,activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fitting Models\n",
    "### a. Fitting Model 1\n",
    "<h3 style=\"color:red;\"><strong>WARNING:</strong></h3>\n",
    "The training can take from <strong>$2$ minutes</strong> to <strong>$10$ minutes </strong>..<br>\n",
    "Maybe have a coffe with your friends?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model.fit(X_mnist_train,y_mnist_train)\n",
    "print(\"Pipeline is now fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Fitting Model 2\n",
    "This may seem like some <strong>cryptic code,</strong> but just a little practice and you will get used to it.<br>\n",
    "This code compile the <strong>CNN</strong> (Convolutional Neural Network):\n",
    "- The Used Mathematical optimiser is the Stochastic Gradient Descent with learning rate $\\alpha=0.01$ and momentum $\\mu=0.9$\n",
    "- The Loss Function $\\mathcal{L}$ is the Categorical Cross Entropy\n",
    "- Let $n$ be the number of observations (<code>n=X.shape[0]</code>), The input $X$ is converted to NumPy array $X'$, rescaled, and converted to the shape $s=(n,28,28,1)$.\n",
    "- $y$ is converted using the function <code>to_categorical</code> to a categorical format $y'$\n",
    "- The model is fit against $X'$ and $y'$\n",
    "\n",
    "<h3 style=\"color:red;\"><strong>WARNING:</strong></h3>\n",
    "The training can take <strong>$10$ minutes </strong>..<br>\n",
    "What about playing some card games?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Invert greyscale of an image, max_col is the maximum value of a colour\n",
    "def invert(img,max_color=255):\n",
    "    return max_color-img\n",
    "\n",
    "#Rescale pixel values to the closed interval [0,1]\n",
    "def rescale(img,max_color=255):\n",
    "    return img/max_color\n",
    "\n",
    "#Add a dummy dimension\n",
    "def convert_img_dataset(X,n1,n2):\n",
    "    return np.reshape(X,[X.shape[0],n1,n2,1]) \n",
    "\n",
    "#This Neural Network will be optimized with Stochastic Gradient Descent (SGC) with learning rate 0.01 and momentum 0.9\n",
    "#The loss function is the Categorical Cross Entropy (Categorical Logistic Loss Function)\n",
    "#The metric of the loss is accuarcy\n",
    "deep_model.compile(optimizer=SGD(lr=0.01,momentum=0.9),\n",
    "              loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "deep_model.fit(convert_img_dataset(rescale(X_mnist_train.to_numpy().astype(np.float32)),28,28),\n",
    "          to_categorical(y_mnist_train),epochs=3,batch_size=32)\n",
    "print(\"CNN Model is now fit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. MNIST Model Evaluation\n",
    "### a. Pipeline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing Dataset Accuarcy: {:.3f}\".format(mnist_model.score(X_mnist_test,y_mnist_test)))\n",
    "mnist_digit_predictor=ImagePredictor(mnist_model,(28,28))\n",
    "print(\"Custom Images Accuarcy: {:.3f}\".format(mnist_digit_predictor.score(custom_images,digit_values,255,invert=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_dummy_dimension(I):\n",
    "    return np.reshape(I,[*I.shape,1])\n",
    "\n",
    "cnn_testing_eval=deep_model.evaluate(rescale(convert_img_dataset(X_mnist_test.to_numpy(),28,28)),to_categorical(y_mnist_test))\n",
    "cnn_custom_eval=deep_model.evaluate(add_dummy_dimension(rescale(invert(custom_images))),to_categorical(digit_values))\n",
    "print(\"Testing Dataset Accuarcy: {:.3f}\".format(cnn_testing_eval[1]))\n",
    "print(\"Custom Images Accuarcy: {:.3f}\".format(cnn_custom_eval[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "1. When we compare the Digits Dataset and MNIST Dataset, we conclude that a model with more features (pixels) will give better results.\n",
    "2. Given <strong>enough data</strong> and <strong>good design</strong>, a Deep Learning Model can easily outperform Classical Machine Learning Algorithms  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
